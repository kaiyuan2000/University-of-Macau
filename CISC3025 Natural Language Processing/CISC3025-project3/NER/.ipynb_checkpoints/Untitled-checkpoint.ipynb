{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'has_(test)': 1}\n"
     ]
    }
   ],
   "source": [
    "features = {}\n",
    "current_word = \"test\"\n",
    "features['has_(%s)' % current_word] = 1\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify.maxent import MaxentClassifier\n",
    "from sklearn.metrics import (accuracy_score, fbeta_score, precision_score,\n",
    "                             recall_score)\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "class MEMM():\n",
    "    def __init__(self):\n",
    "        self.train_path = \"../data/train\"\n",
    "        self.dev_path = \"../data/dev\"\n",
    "        self.beta = 0\n",
    "        self.max_iter = 0\n",
    "        self.classifier = None\n",
    "\n",
    "    def features(self, words, previous_label, position):\n",
    "        \"\"\"\n",
    "        Note: The previous label of current word is the only visible label.\n",
    "\n",
    "        :param words: a list of the words in the entire corpus\n",
    "        :param previous_label: the label for position-1 (or O if it's the start\n",
    "                of a new sentence)\n",
    "        :param position: the word you are adding features for\n",
    "        \"\"\"\n",
    "\n",
    "        features = {}\n",
    "        \"\"\" Baseline Features \"\"\"\n",
    "        current_word = words[position]\n",
    "        features['has_(%s)' % current_word] = 1\n",
    "        features['prev_label'] = previous_label\n",
    "        if current_word[0].isupper():\n",
    "            features['Titlecase'] = 1\n",
    "\n",
    "        #===== TODO: Add your features here =======#\n",
    "        #  ALLCAP[0:-1].isupper()\n",
    "        cap_flag = True\n",
    "        for i in range(len(current_word)):\n",
    "            if current_word[i].islower():\n",
    "                cap_flag = False\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "        if cap_flag==True:\n",
    "            features['ALLCAP'] = 1\n",
    "        \n",
    "        #  all lowercase\n",
    "        low_flag = True\n",
    "        for i in range(len(current_word)):\n",
    "            if current_word[i].isupper():\n",
    "                low_flag = False\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "        if low_flag==True:\n",
    "            features['lowercase'] = 1\n",
    "            \n",
    "        #  after symbols\n",
    "        symbol = ['.',',','\"','\\'','(',')']\n",
    "        if words[position-1] in symbol:\n",
    "            features['symbol'] = 1\n",
    "        \n",
    "        #  have number\n",
    "        if re.match(r'[0-9]', current_word):\n",
    "            features['number'] = 1\n",
    "        \n",
    "        #  prev word is mr. ms. mrs. Mister Mistress Miss President, Minister\n",
    "        pretitle = ['mr.','ms.','mrs.','mister','mistress','miss','president','minister']\n",
    "        if words[position-1].lower() in pretitle:\n",
    "            features['pretitle'] = 1\n",
    "          \n",
    "        #=============== TODO: Done ================#\n",
    "        return features\n",
    "\n",
    "    def load_data(self, filename):\n",
    "        words = []\n",
    "        labels = []\n",
    "        for line in open(filename, \"r\", encoding=\"utf-8\"):\n",
    "            doublet = line.strip().split(\"\\t\")\n",
    "            if len(doublet) < 2:     # remove emtpy lines\n",
    "                continue\n",
    "            words.append(doublet[0])\n",
    "            labels.append(doublet[1])\n",
    "        return words, labels\n",
    "\n",
    "    def train(self):\n",
    "        print('Training classifier...')\n",
    "        words, labels = self.load_data(self.train_path)\n",
    "        previous_labels = [\"O\"] + labels\n",
    "        features = [self.features(words, previous_labels[i], i)\n",
    "                    for i in range(len(words))]\n",
    "        train_samples = [(f, l) for (f, l) in zip(features, labels)]\n",
    "        classifier = MaxentClassifier.train(\n",
    "            train_samples, max_iter=self.max_iter)\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def test(self):\n",
    "        print('Testing classifier...')\n",
    "        words, labels = self.load_data(self.dev_path)\n",
    "        previous_labels = [\"O\"] + labels\n",
    "        features = [self.features(words, previous_labels[i], i)\n",
    "                    for i in range(len(words))]\n",
    "        results = [self.classifier.classify(n) for n in features]\n",
    "\n",
    "        f_score = fbeta_score(labels, results, average='macro', beta=self.beta)\n",
    "        precision = precision_score(labels, results, average='macro')\n",
    "        recall = recall_score(labels, results, average='macro')\n",
    "        accuracy = accuracy_score(labels, results)\n",
    "\n",
    "        print(\"%-15s %.4f\\n%-15s %.4f\\n%-15s %.4f\\n%-15s %.4f\\n\" %\n",
    "              (\"f_score=\", f_score, \"accuracy=\", accuracy, \"recall=\", recall,\n",
    "               \"precision=\", precision))\n",
    "\n",
    "        return True\n",
    "\n",
    "    def show_samples(self, bound):\n",
    "        \"\"\"Show some sample probability distributions.\n",
    "        \"\"\"\n",
    "        words, labels = self.load_data(self.train_path)\n",
    "        previous_labels = [\"O\"] + labels\n",
    "        features = [self.features(words, previous_labels[i], i)\n",
    "                    for i in range(len(words))]\n",
    "        (m, n) = bound\n",
    "        pdists = self.classifier.prob_classify_many(features[m:n])\n",
    "\n",
    "        print('  Words          P(PERSON)  P(O)\\n' + '-' * 40)\n",
    "        for (word, label, pdist) in list(zip(words, labels, pdists))[m:n]:\n",
    "            if label == 'PERSON':\n",
    "                fmt = '  %-15s *%6.4f   %6.4f'\n",
    "            else:\n",
    "                fmt = '  %-15s  %6.4f  *%6.4f'\n",
    "            print(fmt % (word, pdist.prob('PERSON'), pdist.prob('O')))\n",
    "\n",
    "    def dump_model(self):\n",
    "        with open('../model.pkl', 'wb') as f:\n",
    "            pickle.dump(self.classifier, f)\n",
    "\n",
    "    def load_model(self):\n",
    "        with open('../model.pkl', 'rb') as f:\n",
    "            self.classifier = pickle.load(f)\n",
    "\n",
    "    def predict_sentence(self, string):\n",
    "        # split sentence\n",
    "        words = ['.'] + word_tokenize(string)\n",
    "        # First word of the input, it will not be predicted as person\n",
    "        # predict each word in sentence\n",
    "        prev_label = ['O'] \n",
    "        for i in range(len(words)):\n",
    "            prev_label.append('') #(unknown labels)\n",
    "        features = [self.features(words, prev_label[i], i)\n",
    "                    for i in range(len(words))]\n",
    "        results = [self.classifier.classify(n) for n in features]\n",
    "        #print(words[1:])\n",
    "        #print(results[1:])\n",
    "        result_list = []\n",
    "        index = 0\n",
    "        for i in range(1,(len(results))):\n",
    "            result_list.append(words[i])\n",
    "            result_list.append(results[i])\n",
    "        print(result_list)\n",
    "        return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier...\n",
      "  ==> Training (5 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.055\n",
      "             2          -0.06094        0.946\n",
      "             3          -0.05451        0.969\n",
      "             4          -0.04894        0.970\n",
      "         Final          -0.04437        0.981\n"
     ]
    }
   ],
   "source": [
    "BETA = 0.5\n",
    "MAX_ITER = 5\n",
    "BOUND = (0, 20)\n",
    "classifier = MEMM()\n",
    "\n",
    "def train_test():\n",
    "    classifier.max_iter = MAX_ITER\n",
    "    classifier.train()\n",
    "    classifier.dump_model()\n",
    "def dev_test():\n",
    "    try:\n",
    "        classifier.load_model()\n",
    "        classifier.beta = BETA\n",
    "        classifier.test()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "def show_test():\n",
    "    try:\n",
    "        classifier.load_model()\n",
    "        classifier.show_samples(BOUND)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier...\n",
      "  ==> Training (5 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.055\n",
      "             2          -0.09383        0.946\n",
      "             3          -0.08134        0.968\n",
      "             4          -0.07136        0.969\n",
      "         Final          -0.06330        0.969\n",
      "Testing classifier...\n",
      "f_score=        0.8715\n",
      "accuracy=       0.9641\n",
      "recall=         0.7143\n",
      "precision=      0.9642\n",
      "\n",
      "  Words          P(PERSON)  P(O)\n",
      "----------------------------------------\n",
      "  EU               0.0544  *0.9456\n",
      "  rejects          0.0286  *0.9714\n",
      "  German           0.0544  *0.9456\n",
      "  call             0.0286  *0.9714\n",
      "  to               0.0284  *0.9716\n",
      "  boycott          0.0286  *0.9714\n",
      "  British          0.0544  *0.9456\n",
      "  lamb             0.0286  *0.9714\n",
      "  .                0.0281  *0.9719\n",
      "  Peter           *0.4059   0.5941\n",
      "  Blackburn       *0.5057   0.4943\n",
      "  BRUSSELS         0.4977  *0.5023\n",
      "  1996-08-22       0.0286  *0.9714\n",
      "  The              0.0544  *0.9456\n",
      "  European         0.0544  *0.9456\n",
      "  Commission       0.0544  *0.9456\n",
      "  said             0.0258  *0.9742\n",
      "  on               0.0283  *0.9717\n",
      "  Thursday         0.0544  *0.9456\n",
      "  it               0.0286  *0.9714\n"
     ]
    }
   ],
   "source": [
    "train_test()\n",
    "dev_test()\n",
    "show_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "classifier = MEMM()\n",
    "\n",
    "def predict(string):\n",
    "    classifier.load_model()\n",
    "    classifier.predict_sentence(string)\n",
    "    \n",
    "string = \"Patricia isn't a president artist in, kpop team named BLACKPINK in 2020. Tom is in China.\"\n",
    "list_ = predict(string)\n",
    "print(list_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier...\n",
      "  ==> Training (5 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.055\n",
      "             2          -0.06095        0.946\n",
      "             3          -0.05451        0.969\n",
      "             4          -0.04894        0.969\n",
      "         Final          -0.04436        0.981\n",
      "Testing classifier...\n",
      "f_score=        0.9219\n",
      "accuracy=       0.9739\n",
      "recall=         0.7924\n",
      "precision=      0.9780\n",
      "\n",
      "  Words          P(PERSON)  P(O)\n",
      "----------------------------------------\n",
      "  EU               0.0486  *0.9514\n",
      "  rejects          0.0008  *0.9992\n",
      "  German           0.1529  *0.8471\n",
      "  call             0.0008  *0.9992\n",
      "  to               0.0008  *0.9992\n",
      "  boycott          0.0008  *0.9992\n",
      "  British          0.1547  *0.8453\n",
      "  lamb             0.0008  *0.9992\n",
      "  .                0.0002  *0.9998\n",
      "  Peter           *0.4445   0.5555\n",
      "  Blackburn       *0.3947   0.6053\n",
      "  BRUSSELS         0.2494  *0.7506\n",
      "  1996-08-22       0.0000  *1.0000\n",
      "  The              0.1659  *0.8341\n",
      "  European         0.1516  *0.8484\n",
      "  Commission       0.1524  *0.8476\n",
      "  said             0.0008  *0.9992\n",
      "  on               0.0008  *0.9992\n",
      "  Thursday         0.1507  *0.8493\n",
      "  it               0.0008  *0.9992\n"
     ]
    }
   ],
   "source": [
    "train_test()\n",
    "dev_test()\n",
    "show_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
